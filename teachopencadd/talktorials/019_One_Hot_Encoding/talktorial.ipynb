{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T019 · The *One-Hot Encoding* concept\n",
    "\n",
    "Developed in the CADD seminar 2020, Volkamer Lab, Charité/FU Berlin \n",
    "\n",
    "Authors : \n",
    "- Sakshi Misra, CADD seminar 2020, Charité/FU Berlin\n",
    "- Talia B. Kimber, [Volkamer lab](https://volkamerlab.org), Charité\n",
    "- Andrea Volkamer, [Volkamer lab](https://volkamerlab.org), Charité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The aim of the talktorial is to perform one-hot encoding of SMILES structures on a subset of the ChEMBL dataset to gain a deeper understanding on the one-hot encoding concept and why it is useful as a pre-processing step in various machine learning algorithms. \n",
    "\n",
    "<img src=\"./images/logo.png\" width=\"300\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "- Introduction to SMILES and some of its specification rules\n",
    "- Understanding categorical data and its conversion to numeric data for usage in machine learning models \n",
    "- Explanation of one-hot encoding concept and its advantages and disadvantages\n",
    "- Integer encoding and the differences between integer encoding and one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "- Import necessary packages and visualize the dataframe\n",
    "- Apply one-hot encoding using own implementation\n",
    "  - Visualization of one-hot encoded matrix (unequal dimension)\n",
    "- Function defined to pre-process the data and then apply one-hot encoding using own implementation on preprocessed data\n",
    "  - Visualization of one-hot encoded matrix (equal dimension)\n",
    "- Apply one-hot encoding using implementation in [Scikit-learn](https://scikitlearn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "   - Without padding and visualization of the matrix (unequal dimension)\n",
    "   - Padding after one-hot encoding is performed and visualization of the matrix (equal dimension)\n",
    "   - Padding before one-hot encoding performed and visualization of the matrix (equal dimension)\n",
    "- Apply one-hot encoding using implementation in [keras](https://keras.io/)\n",
    "  - Without padding and visualization of the matrix (unequal dimension)\n",
    "  - With padding and visualization of the matrix (equal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Theoretical background:\n",
    "     - ChEMBL database : Bento, A. Patrícia, et al. \"The ChEMBL bioactivity database: an update.\" _Nucleic acids research_ 42.D1 (2014): D1083-D1090. https://doi.org/10.1093/nar/gkt1031.\n",
    "     - Blogpost : Jason Brownlee, _How to One Hot Encode Sequence Data in Python_, Machine Learning Mastery, Available from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/, accessed November 9th, 2020.\n",
    "     \n",
    "\n",
    "- Packages used:\n",
    "     - [**rdkit**](https://www.rdkit.org/docs/GettingStartedInPython.html) : Greg Landrum,  _RDKit Documentation_, [PDF](https://buildmedia.readthedocs.org/media/pdf/rdkit/latest/rdkit.pdf), Release on 2019.09.1.\n",
    "     - [**Scikit-learn**](https://scikit-learn.org/stable/) : Jiangang Hao, et al. \"A Review of Scikit-learn Package in Python Programming Language.\" _Journal of Education and Behavioral Statistics_ Volume: 44 issue: 3 (2019), page(s): 348-361. https://doi.org/10.3102/1076998619832248.\n",
    "     - [**keras**](https://keras.io/) : Book chapter: \"An Introduction to Deep Learning and Keras\" in *Learn Keras for Deep Neural Networks* (2019), page(s):1-16, https://doi.org/10.1007/978-1-4842-4240-7.\n",
    "     - [**Matplotlib**](https://matplotlib.org/)\n",
    "     - [**timeit**](https://docs.python.org/3/library/timeit.html)\n",
    "     \n",
    "\n",
    "- `SMILES encoder` function: Blogpost by iwatobipen, *encode and decode SMILES strings* , Wordpress, Available from https://iwatobipen.wordpress.com/2017/01/22/encode-and-decode-smiles-strings/, accessed January 1st, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChEMBL database\n",
    "\n",
    "- [ChEMBL](https://www.ebi.ac.uk/chembl/) is an open large-scale bioactivity database. \n",
    "- It is a database having molecules with drug-like properties. \n",
    "- Recent release 17 contains information extracted from  more than 51,000 publications, together with bioactivity   data sets from 18 other sources (depositors and databases). In total, there are now more than 1.3 million distinct compound structures and 12 million bioactivity data points.\n",
    "- It is maintained by [European Bioinformatics Institute](https://en.wikipedia.org/wiki/European_Bioinformatics_Institute).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES structures\n",
    "- [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) (Simplified Molecular Input Line Entry System) notation is a chemical notation that allows a user to represent a chemical structure of a molecule in a linear way that can be used by the computer ( see \"Modern Aspects of the Smiles Rearrangement\" (2017), [_Chemistry A European Journal_, <b>Volume23, Issue38</b>, 8992-9008](https://doi.org/10.1002/chem.201700353) for further information. )\n",
    "- It contains a chain of letters, number and characters that specify the atoms, their connectivity , their bond order and chirality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some SMILES specification rules\n",
    "- **Atoms** - are represented by their atomic symbols. Also metal atoms are represented with symbols in square bracket, for eg. Gold `[Au]`.\n",
    "- **Bonds** - Single, Double and Triple bonds are represented by symbols `-`, `=` and `#` respectively.Aromatic bonds are represented by `*`. Single bonds are the default and therefore need not be entered. Aromatic C,O,S and N atoms are shown in lower case like 'c', 'o', 's' and 'n' or by symbo ':' whereas Aliphatic C,O,S and N atoms are shown in upper case. For example, 'CC' would mean that there is a non-aromatic carbon attached to another non-aromatic carbon by a single bond, and the computer would identify the structure as the chemical Ethane (`CH3CH3`).\n",
    "- **Rings** - SMILES allows a user to identify ring structures by using numbers to identify the opening and closing ring atom. For example, in `C1CCCCC1`, the first carbon has a number '1' which connects by a single bond with the last carbon which also has a number '1'. The resulting structure is cyclohexane\n",
    "- **Branches** - are specified by enclosing them in parentheses, and can be nested or arranged. For Eg. 2-Propanol is represented by CC(O)C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Categorical Data?\n",
    "Categorical data are variables that contain label values rather than numeric values.\n",
    "Some examples include:\n",
    "\n",
    "- A “pet” variable with the values: “dog” and “cat“.\n",
    "- A “color” variable with the values: “red“, “green” and “blue“.\n",
    "- A “place” variable with the values: “first”, “second” and “third“.\n",
    "\n",
    "Talking about in terms of bioinformatics, if we are using machine learning classifier to classify cancerous and normal tissues cells, we can have label values say \"Lung Cancer\", \"Breast Cancer\", \"Liver Cancer\" and \"Healthy Controls\".\n",
    "We first need to one-hot encode these categorical label values and then we can apply binary or multi-class classifier to achieve classification results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the problem with categorical data?\n",
    "Machine Learning is, after all, a bunch of mathematical operations translated to a computer via low-level programming languages.Computers are brilliant when dealing with numbers. So, we must somehow convert our input data to numbers. \n",
    "There are many machine learning algorithms which cannot operate on categorical data directly so they must be converted to a numerical form so all our input variables and output variables will be numeric. (see [**Blogpost**](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/): Alakh Sethi, _One-Hot Encoding vs. Label Encoding using Scikit-Learn_, Analytics Vidya, [accessed March 6th, 2020](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/). for further information)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/external-content.duckduckgo.com_.jpeg\" alt=\"Drawing\" style=\"max-width: 500px; width:400%;\"/>\n",
    "</div>\n",
    "\n",
    "**Figure 1** displays the categorical encoding requires for our computers to understand the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to convert categorical data to numerical data?\n",
    "There are many ways to convert categorical values into numerical values.Each approach has its own positive and negative impact on the feature set. Hereby, I would be focusing on 2 main methods: `One-Hot-Encoding` and `Label-Encoder`.\n",
    "Both of these encoders are part of SciKit-learn library (one of the most widely used Python library) and are used to convert text or categorical data into numerical data which the model expects and perform better with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *One-Hot Encoding* concept\n",
    "The one-hot encoding is a vector representation where all the elements of the vector are 0 except one, which has 1 as its value. For example, [0 0 0 1 0 0] is a one-hot vector.\n",
    "Simply, one-hot encoding also known as binary encoding, is a binary representation of categorical variables as binary vectors. \n",
    "\n",
    "Figures shown below helps us to gain an overall idea of one-hot encoding concept, see [<i>BMC Bioinformatics.</i> (2018), <b>19</b>,526](https://doi.org/10.1186/s12859-018-2523-5) for further information.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12859-018-2523-5/MediaObjects/12859_2018_2523_Fig1_HTML.png?as=webp\" style=\"max-width: 500px; width:400%;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**Figure 2a** shows the one-hot encoding of four DNA nucleotides, a filter kernel with one-dimensional CNN, and \n",
    "\n",
    "**Figure 2b** shows the one-hot encoding of toluene and applying one-dimensional CNN to SMILES linear representations of chemical compound toluene.\n",
    " \n",
    "\n",
    "Lets take a deeper look into the concept with the help of a simple example that will describe the basic concept of one-hot encoding, why it is useful and how one can approach towards it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use a one-hot encoding?\n",
    "A one-hot encoding allows the representation of categorical data to be more expressive.\n",
    "Its difficult for many machine learning algorithms to work with categorical data directly that's why the label values which are categorical must be converted into numbers first as a preprocessing step. This is required for both input and output variables that are categorical.\n",
    "We could use an integer encoding directly. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n",
    "There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘dog’ and ‘cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of one-hot encoding\n",
    "Lets take a look at a very simple example to understand this concept.\n",
    "Lets assume we have the “color” variable which has three labels, `RED` , `BLUE` and `GREEN`.\n",
    "All these labels must be converted into numeric form in order to work with our machine learning algorithm, this can be done by creating three new columns having all the three labels and use “1” value for the color and “0” values for the other colors as shown in Figure 3. (see an article: \"*Building a One Hot Encoding Layer with TensorFlow*\", George Novack\n",
    ",[towardsdatascience](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39) for more detail )\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/OneHotEncoding_eg.png\" style=\"max-width: 700px; width:150%;\" />\n",
    "</div>\n",
    "\n",
    "**Figure 3** shows the visual demonstration of one-hot encoding done on variable \"color\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Advantages of using one-hot encoding \n",
    "-  If the cardinality (the number of categories) of the categorical features is low (relative to the amount of data) one-hot encoding will work best. We can use it as input into any machine learning model.\n",
    "-  We can create binary representation of our label values which can be useful for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Disadvantages of using one-hot encoding \n",
    "-  Increase in dimensionality, after adding several columns based on categorical variables, the dataset will be having more dimensions than before and in result it can increase the computational cost.\n",
    "- There is a high chances of multicollinearity due to dummy variables which can affect the performance of our model.\n",
    "-  Increase [Sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) (sparse array is a matrix in which most of the elements are zero, one-hot encoding can result in increasing the sparsity of our dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other similar transformation: Integer encoding (label encoder)\n",
    "\n",
    "This is called a label encoding or an integer encoding and is easily reversible.\n",
    "[Label Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is also a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering, so that machines can work with it properly.\n",
    "Machine learning algorithms can then decide in a better way on how labels must be operated. \n",
    "It is an important preprocessing step for the structured dataset in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of integer encoding\n",
    "Lets take a similar example as above, we have a color variable and we can assign “red” as 0, “green” as 1, and “blue” as 2 as shown in Figure 4, (see an article: \"*Know about Categorical Encoding, even New Ones!*\", Ahmed Othmen, [towardsdatascience](https://towardsdatascience.com/know-about-categorical-encoding-even-new-ones-c266227b9cbd) for more detail )\n",
    "\n",
    "\n",
    "\n",
    "![OneHotEncoding Example](images/label_encoding_example.png)\n",
    "\n",
    "**Figure 4** shows the visual demonstration of label encoding of color variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between label and one-hot encoding\n",
    "\n",
    "There is not much difference between these two encoding techniques, its mainly depends on the type of data and model we are using. For example if we have categorical features which are not ordinal (dog or cat) then we can use one-hot encoding. Label encoding works best with ordinal data like `Good=0, Better=1, Best=2`.\n",
    "Also when there are more categorical variables then its good to choose label encoding just to avoid high memory consumption and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings\n",
    "\n",
    "This section lists some resources for further reading\n",
    "\n",
    "- [What is one-hot encoding and when is it used in data science?](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science)\n",
    "- [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd#:~:text=One%2DHot%20Encoding%20in%20Python&text=OneHotEncoder%20from%20SciKit%20library%20only,apply%20OneHotEncoder%20on%20column%20Bridge_Types_Cat.)\n",
    "- [Research Article: Convolutional neural network based on SMILES representation of compounds for detecting chemical motif](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2523-5) #TODO\n",
    "- [How one can use matplotlib.pyplot.imshow() in Python](https://www.geeksforgeeks.org/matplotlib-pyplot-imshow-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flake8 pycodestyle_magic \n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings for readibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rdkit.Chem import Draw, PandasTools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and draw molecules\n",
    "\n",
    "Using the `Pandas` library, we first load the data and then draw the molecules using the `rdkit.draw` function. Finally we apply different implementations of `one-hot encoding` into the SMILES structures.\n",
    "\n",
    "Let's load the data and quickly analyze its column values and check if there are any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\"\n",
    "\n",
    "df = pd.read_csv('DATA/CHEMBL25_activities_EGFR.csv',\n",
    "                 lineterminator='\\n', index_col=0)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension and missing value of the data\n",
    "print(\"Shape of dataframe : \", df.shape)\n",
    "df.info()\n",
    "\n",
    "# Look at head\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas tools and Draw method,\n",
    "# we can visualize our molecules with their ChEMBL ID\n",
    "PandasTools.AddMoleculeColumnToFrame(df, smilesCol='canonical_smiles')\n",
    "Draw.MolsToGridImage(list(df.ROMol[0:10]),\n",
    "                     legends=list(df.chembl_id[0:20]),\n",
    "                     molsPerRow=5)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column name ROMol to 2D_Figure to get the proper idea of the column\n",
    "df = df.rename(columns={'ROMol': '2D_Figures'})\n",
    "df.head(2)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation\n",
    "\n",
    "We now define our own function which will be useful to create the one-hot encoded matrix of our SMILES strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of all possible SMILES characters\n",
    "SMILES_CHARS = [' ', '#', '%', '(',\n",
    "                ')', '+', '-', '.', '/', '0', '1', '2', '3',\n",
    "                '4', '5', '6', '7', '8', '9',\n",
    "                '=', '@', 'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M',\n",
    "                'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                '[', '\\\\', ']', 'a', 'b', 'c', 'e', 'g', 'i',\n",
    "                'l', 'n', 'o', 'p', 'r', 's', 't', 'u']\n",
    "\n",
    "# Convert the dataset into dictionary\n",
    "smi2index = dict((char, index) for index, char in enumerate(SMILES_CHARS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def smiles_encoder(smiles):\n",
    "    \"\"\"\n",
    "    One-hot encodes SMILES strings\n",
    "    using all possible 56 pre-defined characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "          SMILES string of a compound.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix: ndarray\n",
    "          one-hot encoded matrix of shape\n",
    "          (defined SMILES_CHARS, length of individual SMILES)\n",
    "    \"\"\"\n",
    "    smiles_length = len(smiles)\n",
    "    smiles_matrix = np.zeros((len(SMILES_CHARS), smiles_length), dtype=int)\n",
    "    for index, char in enumerate(smiles):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the canonical SMILES strings\n",
    "start = timer()\n",
    "df['Own_OneHotEncoding'] = df['canonical_smiles'].apply(smiles_encoder)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for the function to execute\n",
    "smiles_encoder_time = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_time:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with Matplotlib.imshow() function\n",
    "`Matplotlib` is a plotting library for the python programming language and `Pyplot` is a state-based interface to a matplotlib module which provides a MATLAB-like interface.\n",
    "The [imshow()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html) function in the pyplot module of the matplotlib library is used to display data as an image i.e. on a 2D space.\n",
    "\n",
    "We now visualize our one-hot encoded matrix using imshow() by defining own `ohe_plot` function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_plot(ohe_matrix, smiles_char):\n",
    "    \"\"\"\n",
    "    Function defined to visualize one-hot encoded matrix\n",
    "    using matplotlib imshow() function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ohe_matrix : ndarray\n",
    "       one-hot encoded matrix of shape\n",
    "       (defined SMILES_CHARS, length individual SMILES)\n",
    "    smiles_char : dict\n",
    "        Dictonary with all possible SMILES characters\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    im = plt.imshow(ohe_matrix, cmap='hot', interpolation='None')\n",
    "    plt.colorbar(im, orientation='horizontal')\n",
    "    plt.xlabel('Length of SMILES string')\n",
    "    plt.ylabel(f'Char in SMILES ({len(smiles_char)})')\n",
    "    plt.title('Visualization of one-hot encoded matrix')\n",
    "    plt.show()\n",
    "    print('Shape of one-hot matrix : ', ohe_matrix.shape)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['Own_OneHotEncoding'], SMILES_CHARS)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated canonical SMILES: ', df.iloc[0]['canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[1]['Own_OneHotEncoding'], SMILES_CHARS)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated canonical SMILES: ', df.iloc[1]['canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, as we notice from the visualizations above, SMILES strings could have unequal dimension since their string length might differ. For machine learning application, having equal dimension throughout the data set is required. In order to achieve this, we first search for the string with the maximum length using the [len()](https://www.geeksforgeeks.org/python-string-length-len/) method. By implementing the above method, we can get the maximum length of the SMILES strings and pass it as an argument in our function for all the strings.\n",
    "\n",
    "- Second, in the above function, we have created a dataset of 56 possible SMILES characters, but to optimize our one hot encoding, we reconsidered using all the 56 characters. We can assume that all the characters won't be present in our SMILES structures, so we look for all the unique characters present in the SMILES data set.\n",
    "\n",
    "- Third, many elements in the periodic table have two characters in their name. For example 'Cl' stands for chloride. This double-character phenomenon is commonly present in our SMILES structure but if we use the above `smiles_encoder` function then we will be splitting 'Cl' into two characters, namely 'C' and 'l', and that would lead to discrepancies, so searching this way for each unique characters and encoding them may not be the best possible way. Hence, we search for all the two alphabetic element in our SMILES by comparing the atoms present in our strings with all the possible elements present in the periodic table and replaced all the two alphabetic elements with one character, for example 'Cl' changed to 'L'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function defined to pre-process the SMILES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function used to preprocessed the data\n",
    "def preprocessing_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the SMILES structures in a data set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "       dataframe which requires preprocessing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with new processed_canonical_smiles column\n",
    "    unique_char : list\n",
    "        list with unique characters present in SMILES\n",
    "    max_length : int\n",
    "        maximum length of strings of canonical_smiles column\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate max length of the SMILES strings\n",
    "    max_len = df[\"canonical_smiles\"].str.len().max()\n",
    "\n",
    "    # Search for unique characters in our SMILES strings\n",
    "    unique_char = set(df.canonical_smiles.apply(list).sum())\n",
    "    upper_chars = ['C', 'O', 'F', 'P', 'N', 'S', 'H', 'B', 'I']\n",
    "    lower_chars = ['l', 'o', 'r', 'n', 'e', 'c', 's']\n",
    "\n",
    "    # List of all possible periodic elements\n",
    "    periodic_elements = ['Ac',\n",
    "                         'Al', 'Am', 'Sb', 'Ar', 'As', 'At', 'Ba',\n",
    "                         'Bk', 'Be', 'Bi', 'Bh', 'B', 'Br', 'Cd', 'Ca',\n",
    "                         'Cf', 'C', 'Ce', 'Cs', 'Cl', 'Cr', 'Co', 'Cn',\n",
    "                         'Cu', 'Cm', 'Ds', 'Db', 'Dy', 'Es', 'Er', 'Eu',\n",
    "                         'Fm', 'Fl', 'F', 'Fr', 'Gd', 'Ga', 'Ge', 'Au',\n",
    "                         'Hf', 'Hs', 'He', 'Ho', 'H', 'In', 'I', 'Ir',\n",
    "                         'Fe', 'Kr', 'La', 'Lr', 'Pb', 'Li', 'Lv',\n",
    "                         'Lu', 'Mg', 'Mn', 'Mt', 'Md', 'Hg', 'Mo',\n",
    "                         'Mc', 'Nd', 'Ne', 'Np', 'Ni', 'Nh', 'Nb',\n",
    "                         'N', 'No', 'Og', 'Os', 'O', 'Pd', 'P',\n",
    "                         'Pt', 'Pu', 'Po', 'K', 'Pr', 'Pm', 'Pa',\n",
    "                         'Ra', 'Rn', 'Re', 'Rh', 'Rg', 'Rb', 'Ru', 'Rf',\n",
    "                         'Sm', 'Sc', 'Sg', 'Se', 'Si', 'Ag', 'Na',\n",
    "                         'Sr', 'S', 'Ta', 'Tc', 'Te', 'Ts', 'Tb', 'Tl', 'Th',\n",
    "                         'Tm', 'Sn',\n",
    "                         'Ti', 'W', 'U', 'V', 'Xe', 'Yb', 'Y', 'Zn', 'Zr']\n",
    "\n",
    "    # 'two_charc_elements' is a list that contains 2 letter elements\n",
    "    # which are valid when compared with all the possible periodic elements.\n",
    "    two_charc_elements = []\n",
    "    for upper in upper_chars:\n",
    "        for lower in lower_chars:\n",
    "            ch = upper + lower\n",
    "            if (ch in periodic_elements):\n",
    "                two_charc_elements.append(ch)\n",
    "\n",
    "    # 'two_charc_elements_smiles' is a set that contains all the possible\n",
    "    # 2 letter elements in our SMILES strings, that is specific to our dataset.\n",
    "    two_charc_elements_smiles = set()\n",
    "    for char in two_charc_elements:\n",
    "        for index in range(len(df)):\n",
    "            if (df['canonical_smiles'].iloc[index].find(char) != -1):\n",
    "                two_charc_elements_smiles.add(char)\n",
    "    # Create a new column having processed canonical SMILES\n",
    "    df['processed_canonical_smiles'] = \"\"\n",
    "\n",
    "    # Replaced all the two letter elements found\n",
    "    # (Cl, Br, Cn, Se, @@) with one character\n",
    "    for index in range(len(df)):\n",
    "        element = df['canonical_smiles'].iloc[index]\n",
    "        element = element.replace(\"Cl\", \"L\")\n",
    "        element = element.replace(\"Br\", \"R\")\n",
    "        element = element.replace(\"Cn\", \"X\")\n",
    "        element = element.replace(\"Se\", \"Z\")\n",
    "        element = element.replace(\"@@\", \"$\")\n",
    "        df['processed_canonical_smiles'].iloc[index] = element\n",
    "    unique_char = set(df.processed_canonical_smiles.apply(list).sum())\n",
    "    return unique_char, df, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This function is highly specific to the above dataset used, because we have replaced two letter elements that are particularly found in the used dataset. One can add more two letter elements present in their specific SMILES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation on preprocessed data\n",
    "\n",
    "Now we have used the same function 'smiles_encoder' but on processed canonical strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function\n",
    "unique_char, df, max_len = preprocessing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of the unique char datset\n",
    "smi2index = dict((char, index) for index, char in enumerate(unique_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def smiles_encoder(smiles, maxlen):\n",
    "    \"\"\"\n",
    "    Function defined using all unique characters in our\n",
    "    processed canonical smiles structures created\n",
    "    from preprocessed function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "         SMILES data in string format\n",
    "    maxlen : int\n",
    "         maximum length of the SMILES string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix : ndarray\n",
    "         one-hot encoded matrix of fixed shape\n",
    "         (unique char in smiles, max smile_length)\n",
    "    \"\"\"\n",
    "    smiles_matrix = np.zeros((len(unique_char), maxlen))\n",
    "    for index, char in enumerate(smiles, 0):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the SMILES strings\n",
    "start = timer()\n",
    "df['UniqueChar_OneHotEncoding'] = df['processed_canonical_smiles'].apply(smiles_encoder,\n",
    "                                                                         maxlen=max_len)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for the function to execute\n",
    "smiles_encoder_equal_dim = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_equal_dim:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['UniqueChar_OneHotEncoding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[1]['UniqueChar_OneHotEncoding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualizations, we can conclude that the dimensions are equal for all the strings' one-hot encoded matrix using `preprocessing_data` and `smiles_encoder` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in Scikit-learn\n",
    "Now, we proceed with our second implementation of one-hot encoding from scikit-learn. We can use [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from the `sklearn` library but it only takes numerical categorical values, hence any value of string type should be [label_encoded](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) first before one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the SMILES into characters\n",
    "def split(string_data):\n",
    "    \"\"\"\n",
    "    Function used to split the SMILES strings into characters array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string_data : str\n",
    "        SMILES string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of characters in the string data\n",
    "    \"\"\"\n",
    "    return [char for char in string_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-learn implementation of one-hot encoding\n",
    "def sklearn_OneHotEncode(smiles, islaterpadding):\n",
    "    \"\"\"\n",
    "    Labels and one-hot encodes the SMILES\n",
    "    using sklearn LabelEncoder and OneHotEncoder implementation\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        smiles\n",
    "    islaterpadding : Bool\n",
    "        ???TODO\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    onehot_encoded: ndarray\n",
    "        one-hot encoded matrix of shape(chars in individual SMILES, length of individual SMILES)\n",
    "    \"\"\"\n",
    "    # Integer encoding\n",
    "    canonical_char = split(smiles)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(canonical_char)\n",
    "    # one-hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    if (islaterpadding == True):\n",
    "        onehot_encoded = laterPadding(onehot_encoded)\n",
    "    onehot_encoded = onehot_encoded.transpose()\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions above first give integer encoded SMILES of the labels and finally one-hot encode the SMILES structures.\n",
    "\n",
    "\n",
    "By default, the OneHotEncoder class will return a more efficient sparse encoding which can be useful in some applications but in this case, we disabled the sparse return type by setting the `sparse=False` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)\n",
    "\n",
    "We can use above defined sklearn_OneHotEncode function to create OneHotEncoded matrix but this will again create unequal dimensions of the matrix because it will first label encode all the characters present in the SMILES strings (individually) and then one-hot encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['sklearn_OneHotEncoded_WP'] = df['processed_canonical_smiles'].apply(sklearn_OneHotEncode, islaterpadding=False)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_without_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_without_padding:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['sklearn_OneHotEncoded_WP'], df.iloc[0]['sklearn_OneHotEncoded_WP'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[100]['sklearn_OneHotEncoded_WP'], df.iloc[100]['sklearn_OneHotEncoded_WP'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be confirmed from above visualization that the our one-hot encoded matrix from sklearn implementation do not have equal dimensions because sklearn Onehot_encoder takes individual strings as their input.\n",
    "\n",
    "So we thought of making equal dimension of our one-hot encoded matrix by adding **padding** which simply means adding zeros to it. It can be either done after one-hot encoding is performed on the SMILES strings or before one-hot encoding is performed, after we label encode our SMILES characters.\n",
    "\n",
    "We will be discussing both the scenarios in next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding after one-hot encoding is perfomed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe easier to use np.pad:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.pad.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined function to add padding after one-hot encoding\n",
    "def later_padding(ohe_matrix):\n",
    "    \"\"\"\n",
    "    Function defined to add horizontal and vertical\n",
    "    padding to the given matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ohe_matrix: ndarray\n",
    "       Character array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_matrix: ndarray\n",
    "       padded one hot encoded matrix of\n",
    "       shape(unique char in smiles, max smile_length)\n",
    "  \"\"\"\n",
    "    index_padding = np.ndarray(shape=(max_len-len(ohe_matrix),\n",
    "                                     len(ohe_matrix[0])))\n",
    "    index_padding.fill(0)\n",
    "    column_padding = np.ndarray(shape=(max_len,\n",
    "                                      len(unique_char)-len(ohe_matrix[0])))\n",
    "    column_padding.fill(0)\n",
    "    ohe_matrix = np.append(matrix, index_padding, axis=0)\n",
    "    ohe_matrix = np.append(matrix, column_adding, axis=1)\n",
    "    return ohe_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['sklearn_OneHotEncoded_later'] = df['processed_canonical_smiles'].apply(sklearn_OneHotEncode, islaterpadding=True)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_Later = end - start\n",
    "print(f'{sklearn_time_Later:.2f} secs') # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualization with Matplotlib.imshow() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['sklearn_OneHotEncoded_later'], unique_char)# NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[1]['sklearn_OneHotEncoded_later'], unique_char)# NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that the dimensions are equal for all the SMILES strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding before one-hot encoding is perfomed\n",
    "\n",
    "In this case we have added padding just after the label encoding or before the one-hot encoding is performed on SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined function to add padding before one-hot encoding\n",
    "def initial_padding(canonical_char, max_len):\n",
    "    \"\"\"\n",
    "    Function is defined to add padding to the given list\n",
    "    before one-hot encoding is performed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical_char : array\n",
    "       Character array\n",
    "    max_len : int\n",
    "       Maximum length of smiles string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_list : list\n",
    "       padded character list of smiles\n",
    "    \"\"\"\n",
    "    zeros = [0] * (max_len-len(canonical_char))\n",
    "    padded_list = canonical_char + zeros\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe easier to use np.pad:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.pad.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_padding_and_encoding(smiles):\n",
    "    \"\"\"\n",
    "    Function defined to one-hot encode initially padded SMILES strings\n",
    "    using sklearn_OneHotEncode function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "       SMILES string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "      one hot encoded with intial padding of shape\n",
    "      (unique char in smiles, max smile_length)\n",
    "    \"\"\"\n",
    "    # call 'split' function on smiles\n",
    "    canonical_char = split(smiles)\n",
    "    # call 'initial_padding' function on splitted list of characters\n",
    "    canonical_char_padded = initial_padding(canonical_char, max_len)\n",
    "    # call 'sklearn_OneHotEncode' function\n",
    "    sklearnOHC = sklearn_OneHotEncode(canonical_char_padded, False)\n",
    "    sklearnOHC = sklearnOHC.transpose()\n",
    "    column_padding = np.ndarray(shape=(max_len,\n",
    "                                      len(unique_char)-len(sklearnOHC[0])))\n",
    "    column_padding.fill(0)\n",
    "    sklearnOHC = np.append(sklearnOHC, column_padding, axis=1)\n",
    "    return sklearnOHC.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['sklearn_OneHotEncoded_initial'] = df['processed_canonical_smiles'].apply(initial_padding_and_encoding)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_initial = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_initial:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualization with Matplotlib.imshow() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['sklearn_OneHotEncoded_initial'], unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[1]['sklearn_OneHotEncoded_initial'], unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is also a very powerful and highly used library, especially employed in for deep learning tasks. \n",
    "There may be a case where we have sequences or strings that are already integer encoded, then in that case we can use function called [to_categorical()](https://keras.io/api/utils/) provide by keras library to one-hot encode integer data directly, but it always should be integer which may not have a real ordinal relationship and are really just placeholders for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keras implementation of one-hot encoding\n",
    "def keras_OneHotEncode(smiles, islaterpadding):\n",
    "    \"\"\"\n",
    "    Function is defined to one-hot encode the SMILES using keras\n",
    "    implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical_char : array\n",
    "        Canonical character array\n",
    "    islaterpadding : bool\n",
    "        ?????\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    encoded : ndarray\n",
    "        one-hot encoded matrix of shape(chars in SMILES, length of SMILES)\n",
    "    \"\"\"\n",
    "    # apply 'split' function on smiles string\n",
    "    canonical_char = split(smiles)\n",
    "    # integer encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(canonical_char)\n",
    "    # one-hot encoding\n",
    "    encoded = to_categorical(integer_encoded)\n",
    "    if (islaterpadding == True):\n",
    "        encoded = laterPadding(encoded)\n",
    "    encoded = encoded.transpose()\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['keras_OneHotEncoded'] = df['processed_canonical_smiles'].apply(keras_OneHotEncode, islaterpadding = False)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time = end - start\n",
    "print(f'{keras_time:.2f} secs') # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization with Matplotlib.imshow() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['keras_OneHotEncoded'], df.iloc[0]['keras_OneHotEncoded'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[100]['keras_OneHotEncoded'], df.iloc[100]['keras_OneHotEncoded'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['keras_OneHotEncoded_padding'] = df['processed_canonical_smiles'].apply(keras_OneHotEncode, islaterpadding = True)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time_WP = end - start\n",
    "print(f'{keras_time_WP:.2f} secs')  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['keras_OneHotEncoded_padding'], unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[1]['keras_OneHotEncoded_padding'], unique_char)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe with all the one-hot encoded matrices\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "\n",
    "As we notice from the simulations above, the execution time varies with different implementations:\n",
    "\n",
    "- **Unequal dimension** (when no padding was performed)\n",
    "    - Unexpectedly, our own `smiles_encoder` function works the best with 0.89 sec execution time followed by the implementation in keras with 1.48 sec and One Hot Encoding with the scikit-learn implementation executed with 2.42 secs, the highest. But since the dimensions differ, machine learning models cannot be applied.\n",
    "    \n",
    "- **Equal dimension** (when padding was performed)\n",
    "     - Surprisingly, even after creating equal dimensions (adding padding), my own functions `smiles_encoder` along with `preprocessing_data` again outperform the other two implementations with execution time of 1.02 secs whereas scikit-learn required approximately 3.0 secs and keras 1.85 secs. One explanation for this difference may be the additional padding performed on the strings.\n",
    "     \n",
    "     \n",
    "We also stress the time execution difference using scikit-learn when padding is performed:\n",
    "\n",
    "- **Before one-hot encoding** : Time required is 3.0 secs approx.\n",
    "- **After one-hot encoding** : Time required is 2.7 secs approx.\n",
    "\n",
    "One possible reason of this time difference could be that if we perform padding after the label encoding, then we will have more characters to one hot encode as compared to just add padding after one encoded is accomplished.\n",
    "\n",
    "**Note:** Execution times might differ depending on the environment used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were several challenges faced during the task, for instance:\n",
    "\n",
    "- Making equal dimensions of the one-hot encoded matrix.\n",
    "\n",
    "- Replacing two letter elements with unique characters.\n",
    "\n",
    "- After searching for two letter elements, we found a specific element `Sc` which is a metallic element, but in our dataset both 'S' and 'c' elements are present individually as well. So if we would have replaced `Sc` with single letter element then it might have effected our actual strings. So we assumed that since `Sc` is a metallic element and its rarely present in SMILES, so we did not replace this element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why is it required to have equal dimensions of the one-hot encoded matrix ?\n",
    "- Is there any other way to pre-process the data ?\n",
    "- How and which machine learning models can be applied on the above dataset ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
