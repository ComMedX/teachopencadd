{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T019 · The *One-Hot Encoding* concept\n",
    "\n",
    "Developed in the CADD seminar 2020, Volkamer Lab, Charité/FU Berlin \n",
    "\n",
    "Authors :\n",
    "\n",
    "- Sakshi Misra, CADD seminar 2020, Charité/FU Berlin\n",
    "- Talia B. Kimber, [Volkamer lab](https://volkamerlab.org), Charité\n",
    "- Andrea Volkamer, [Volkamer lab](https://volkamerlab.org), Charité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The aim of the talktorial is to perform one-hot encoding of SMILES structures on a subset of the ChEMBL dataset to gain a deeper understanding on the one-hot encoding concept and why it is useful as a pre-processing step in various machine learning algorithms. \n",
    "\n",
    "<img src=\"./images/logo.png\" width=\"300\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "- Introduction to SMILES and some of its specification rules\n",
    "- Understanding categorical data and its conversion to numeric data for usage in machine learning models \n",
    "- Explanation of one-hot encoding concept and its advantages and disadvantages\n",
    "- Integer encoding and the differences between integer encoding and one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "- Import necessary packages and visualize the dataframe\n",
    "- Apply one-hot encoding using own implementation\n",
    "  - Visualization of one-hot encoded matrix (unequal dimension)\n",
    "- Function defined to pre-process the data and then apply one-hot encoding using own implementation on preprocessed data\n",
    "  - Visualization of one-hot encoded matrix (equal dimension)\n",
    "- Apply one-hot encoding using implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "   - Without padding and visualization of the matrix (unequal dimension)\n",
    "   - Padding after one-hot encoding is performed and visualization of the matrix (equal dimension)\n",
    "   - Padding before one-hot encoding performed and visualization of the matrix (equal dimension)\n",
    "- Apply one-hot encoding using implementation in [keras](https://keras.io/)\n",
    "  - Without padding and visualization of the matrix (unequal dimension)\n",
    "  - With padding and visualization of the matrix (equal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Theoretical background:\n",
    "     - ChEMBL database: Bento, A. Patrícia, et al. \"The ChEMBL bioactivity database: an update.\" _Nucleic acids research_ 42.D1 (2014): D1083-D1090. https://doi.org/10.1093/nar/gkt1031.\n",
    "     - Blogpost: Jason Brownlee, _How to One Hot Encode Sequence Data in Python_, Machine Learning Mastery, Available from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/, accessed November 9th, 2020.\n",
    "     \n",
    "\n",
    "- Packages used:\n",
    "     - [**rdkit**](https://www.rdkit.org/docs/GettingStartedInPython.html): Greg Landrum,  _RDKit Documentation_, [PDF](https://buildmedia.readthedocs.org/media/pdf/rdkit/latest/rdkit.pdf), Release on 2019.09.1.\n",
    "     - [**scikit-learn**](https://scikit-learn.org/stable/):\n",
    "         - [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa _et al._, JMLR 12, pp. 2825-2830, 2011.\n",
    "         - Jiangang Hao, et al. \"A Review of Scikit-learn Package in Python Programming Language.\" _Journal of Education and Behavioral Statistics_ Volume: 44 issue: 3 (2019), page(s): 348-361. https://doi.org/10.3102/1076998619832248.\n",
    "     - [**keras**](https://keras.io/): Book chapter: \"An Introduction to Deep Learning and Keras\" in *Learn Keras for Deep Neural Networks* (2019), page(s):1-16, https://doi.org/10.1007/978-1-4842-4240-7.\n",
    "     - [**Matplotlib**](https://matplotlib.org/)\n",
    "     - [**timeit**](https://docs.python.org/3/library/timeit.html)\n",
    "     \n",
    "\n",
    "- `SMILES encoder` function: Blogpost by iwatobipen, *encode and decode SMILES strings* , Wordpress, Available from https://iwatobipen.wordpress.com/2017/01/22/encode-and-decode-smiles-strings/, accessed November 9th, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChEMBL database\n",
    "\n",
    "- [ChEMBL](https://www.ebi.ac.uk/chembl/) is an open large-scale bioactivity database, containing molecules with drug-like properties. \n",
    "- Recent release 17 contains information extracted from  more than 51,000 publications, together with bioactivity   data sets from 18 other sources (depositors and databases). In total, there are now more than 1.3 million distinct compound structures and 12 million bioactivity data points.\n",
    "- It is maintained by [European Bioinformatics Institute](https://en.wikipedia.org/wiki/European_Bioinformatics_Institute).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES structures\n",
    "- [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) (Simplified Molecular Input Line Entry System) notation is a chemical notation that allows a user to represent a chemical structure of a molecule in a linear way that can be read by the computer (see \"Modern Aspects of the Smiles Rearrangement\" (2017), [_Chemistry A European Journal_, **Volume23, Issue38**, 8992-9008](https://doi.org/10.1002/chem.201700353) for further information.)\n",
    "- It contains a chain of letters, numbers and characters that specify the atoms, their connectivity, their bond order and chirality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some SMILES specification rules\n",
    "- **Atoms** - are represented by their atomic symbols. Also metal atoms are represented with symbols in square bracket, e.g. Gold `[Au]`.\n",
    "- **Bonds** - single, double and triple bonds are represented by symbols `-`, `=` and `#` respectively. Aromatic bonds are represented by `*`. Single bonds are the default and therefore need not be entered. Aromatic C, O, S and N atoms are shown in lower case like 'c', 'o', 's' and 'n' or by symbol `:`, whereas aliphatic C, O, S and N atoms are shown using upper case. For example, 'CC' would mean that there is a non-aromatic carbon attached to another non-aromatic carbon by a single bond, and the computer would identify the structure as the chemical ethane `CH3CH3`.\n",
    "- **Rings** - SMILES allows a user to identify ring structures by using numbers to identify the opening and closing ring atom. For example, in `C1CCCCC1`, the first carbon has a number '1' which connects by a single bond with the last carbon which also has a number '1'. The resulting structure is cyclohexane.\n",
    "- **Branches** - are specified by enclosing them in parentheses, and can be nested or arranged. For example, 2-Propanol is represented by `CC(O)C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Categorical Data?\n",
    "Categorical data are variables that contain label values rather than numeric values.\n",
    "Some examples include:\n",
    "\n",
    "- A “pet” variable with the values: “dog” and “cat“.\n",
    "- A “color” variable with the values: “red“, “green” and “blue“.\n",
    "- A “place” variable with the values: “first”, “second” and “third“.\n",
    "\n",
    "In terms of bioinformatics, if we are using a machine learning classifier to classify cancerous and normal tissues cells, we can have label values say \"Lung Cancer\", \"Breast Cancer\", \"Liver Cancer\" and \"Healthy Controls\".\n",
    "We first need to one-hot encode these categorical label values and then we can apply binary or multi-class classifiers to achieve classification results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the problem with categorical data?\n",
    "Machine Learning is, after all, a bunch of mathematical operations translated to a computer via low-level programming languages. Computers are brilliant when dealing with numbers. So we must somehow convert our input data to numbers. \n",
    "There are many machine learning algorithms which cannot operate on categorical data directly so they must be converted to a numerical form so all our input variables and output variables will be numeric (see [**Blogpost**](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/): Alakh Sethi, _One-Hot Encoding vs. Label Encoding using Scikit-Learn_, Analytics Vidya, accessed March 6th, 2020 for further information).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/external-content.duckduckgo.com_.jpeg\" alt=\"Drawing\" style=\"max-width: 500px; width:400%;\"/>\n",
    "</div>\n",
    "\n",
    "**Figure 1** displays the categorical encoding required for computers to understand the input. The figure comes from this [blogpost](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to convert categorical data to numerical data?\n",
    "There are many ways to convert categorical values into numerical values. Each approach has its own positive and negative impact on the feature set. Hereby, two main methods will be the focus: `One-Hot` encoding and `Label` encoding.\n",
    "Both of these encoders are part of the scikit-learn library (one of the most widely used Python libraries) and are used to convert text or categorical data into numerical data which the model expects and can perform with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *One-Hot Encoding* concept\n",
    "The one-hot encoding is a vector representation where all the elements of the vector are set to `0` except one, which has `1` as its value. For example, `[0 0 0 1 0 0]` is a one-hot vector.\n",
    "Simply put, one-hot encoding, also known as binary encoding, is a binary representation of categorical variables as binary vectors. \n",
    "\n",
    "The figures shown below help us gain an overall idea of the one-hot encoding concept, see [<i>BMC Bioinformatics.</i> (2018), <b>19</b>,526](https://doi.org/10.1186/s12859-018-2523-5) for further information.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12859-018-2523-5/MediaObjects/12859_2018_2523_Fig1_HTML.png?as=webp\" style=\"max-width: 500px; width:400%;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**Figure 2a** shows the one-hot encoding of four DNA nucleotides, a filter kernel with one-dimensional convolutional neural network.\n",
    "\n",
    "**Figure 2b** shows the one-hot encoding of toluene and applying a one-dimensional convolutional neural network to the SMILES linear representation of the chemical compound toluene.\n",
    " \n",
    "\n",
    "Let us take a deeper look into the concept with the help of a simple example that will describe the basic concept of one-hot encoding, why it is useful and how one can approach it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use one-hot encoding?\n",
    "One-hot encoding allows the representation of categorical data to be more expressive.\n",
    "It is difficult for many machine learning algorithms to work with categorical data directly that's why the label values which are categorical must be converted into numbers first as a preprocessing step. This is required for both input and output variables that are categorical.\n",
    "We could use an integer encoding directly. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n",
    "There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘dog’ and ‘cat’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of one-hot encoding\n",
    "Let us take a look at a very simple example to understand this concept. Assume we have the “color” variable which has three labels `RED` , `BLUE` and `GREEN`.\n",
    "All these labels must be converted into numeric form in order to work with our machine learning algorithm. This can be done by creating three new columns having all three labels and use `1` for the color and `0` for the other colors as shown in Figure 3 (see article: \"*Building a One Hot Encoding Layer with TensorFlow*\", George Novack, [towardsdatascience](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39) for more details).\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/OneHotEncoding_eg.png\" style=\"max-width: 700px; width:150%;\" />\n",
    "</div>\n",
    "\n",
    "**Figure 3** shows the visual demonstration of one-hot encoding done on the variable \"color\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Advantages of one-hot encoding \n",
    "-  If the cardinality (the number of categories) of the categorical features is low (relative to the amount of data), one-hot encoding will work best. We can use it as input to any machine learning model.\n",
    "-  We can create binary representations of label values which can be useful for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Disadvantages of one-hot encoding \n",
    "-  Increase in dimensionality, after adding several columns based on categorical variables. The dataset will have more dimensions than before and in result may increase the computational cost.\n",
    "- There is a high chance of multicollinearity due to dummy variables which can affect the performance of our model.\n",
    "-  Increase [sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) (a sparse array is a matrix in which most of the elements are zero, one-hot encoding can result in increasing the sparsity of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other similar transformation: Integer encoding (label encoder)\n",
    "\n",
    "[Label Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), or integer encoding, is a popular encoding technique for handling categorical variables and is easily reversible. In this technique, each label is assigned a unique integer based on alphabetical ordering, so that machines can work with it properly.\n",
    "Machine learning algorithms can then decide in a better way on how labels must be operated on.\n",
    "It is an important preprocessing step for the structured dataset in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of integer encoding\n",
    "Let us take a similar example as above: we have a color variable and we can assign `red` as `0`, `green` as `1`, and `blue` as `2` as shown in Figure 4 (see article: \"*Know about Categorical Encoding, even New Ones!*\", Ahmed Othmen, [towardsdatascience](https://towardsdatascience.com/know-about-categorical-encoding-even-new-ones-c266227b9cbd) for more details).\n",
    "\n",
    "\n",
    "\n",
    "![OneHotEncoding Example](images/label_encoding_example.png)\n",
    "\n",
    "**Figure 4** shows the visual demonstration of label encoding of color variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between label and one-hot encoding\n",
    "\n",
    "There is not much difference between these two encoding techniques, it mainly depends on the type of data and model we are using. For example if we have categorical features which are not ordinal (dog or cat) then we can use one-hot encoding. Label encoding works best with ordinal data like `Good=0, Better=1, Best=2`.\n",
    "Also when there are more categorical variables then its good to choose label encoding just to avoid high memory consumption and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings\n",
    "\n",
    "This section lists some resources for further reading:\n",
    "\n",
    "- [What is one-hot encoding and when is it used in data science?](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science)\n",
    "- [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd#:~:text=One%2DHot%20Encoding%20in%20Python&text=OneHotEncoder%20from%20SciKit%20library%20only,apply%20OneHotEncoder%20on%20column%20Bridge_Types_Cat.)\n",
    "- Hirohara, M., Saito, Y., Koda, Y. et al. Convolutional neural network based on SMILES representation of compounds for detecting chemical motif. _BMC Bioinformatics_ **19**, 526 (2018). https://doi.org/10.1186/s12859-018-2523-5.\n",
    "- [How one can use matplotlib.pyplot.imshow() in Python](https://www.geeksforgeeks.org/matplotlib-pyplot-imshow-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings for readibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rdkit.Chem import Draw, PandasTools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and draw molecules\n",
    "\n",
    "Using the `Pandas` library, we first load the data and then draw the molecules using the `rdkit.draw` function. Then we apply different implementations of `one-hot encoding` to the SMILES structures.\n",
    "\n",
    "Let's load the data and quickly analyze its column values and check if there are any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\"\n",
    "\n",
    "df = pd.read_csv('DATA/CHEMBL25_activities_EGFR.csv',\n",
    "                 lineterminator='\\n', index_col=0)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension and missing value of the data\n",
    "print(\"Shape of dataframe : \", df.shape)\n",
    "df.info()\n",
    "\n",
    "# Look at head\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas tools and the Draw method,\n",
    "# we can visualize the molecules with their ChEMBL ID\n",
    "PandasTools.AddMoleculeColumnToFrame(df, smilesCol='canonical_smiles')\n",
    "Draw.MolsToGridImage(list(df.ROMol[0:10]),\n",
    "                     legends=list(df.chembl_id[0:20]),\n",
    "                     molsPerRow=5)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column name ROMol to 2D_Figure to get the proper idea of the column\n",
    "df = df.rename(columns={'ROMol': '2D_Figures'})\n",
    "df.head(2)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation\n",
    "\n",
    "We now define our own function which will be useful to create the one-hot encoded matrix of the SMILES strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of all possible SMILES characters\n",
    "SMILES_CHARS = [' ', '#', '%', '(',\n",
    "                ')', '+', '-', '.', '/', '0', '1', '2', '3',\n",
    "                '4', '5', '6', '7', '8', '9',\n",
    "                '=', '@', 'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M',\n",
    "                'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                '[', '\\\\', ']', 'a', 'b', 'c', 'e', 'g', 'i',\n",
    "                'l', 'n', 'o', 'p', 'r', 's', 't', 'u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a dictionary\n",
    "smi2index = dict((char, index) for index, char in enumerate(SMILES_CHARS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def smiles_encoder(smiles):\n",
    "    \"\"\"\n",
    "    One-hot encodes SMILES strings\n",
    "    using all possible 56 pre-defined characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "          SMILES string of a compound.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix: ndarray\n",
    "          One-hot encoded matrix of shape\n",
    "          (defined SMILES_CHARS, length of individual SMILES).\n",
    "    \"\"\"\n",
    "    smiles_length = len(smiles)\n",
    "    smiles_matrix = np.zeros((len(SMILES_CHARS), smiles_length), dtype=int)\n",
    "    for index, char in enumerate(smiles):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the canonical SMILES strings\n",
    "start = timer()\n",
    "df['own_OHE'] = df['canonical_smiles'].apply(smiles_encoder)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "smiles_encoder_time = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_time:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with Matplotlib.imshow() function\n",
    "`Matplotlib` is a plotting library for the python programming language and `Pyplot` is a state-based interface to a matplotlib module which provides a MATLAB-like interface.\n",
    "The [imshow()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html) function in the pyplot module of the matplotlib library is used to display data as an image i.e. on a 2D space.\n",
    "\n",
    "We now visualize our one-hot encoded matrix using `imshow()` by defining our own `ohe_plot` function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_plot(ohe_matrix, smiles_char):\n",
    "    \"\"\"\n",
    "    Visualize one-hot encoded matrix\n",
    "    using matplotlib imshow() function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ohe_matrix : ndarray\n",
    "       One-hot encoded matrix of shape\n",
    "       (defined SMILES_CHARS, length individual SMILES).\n",
    "    smiles_char : dict\n",
    "        Dictonary with all possible SMILES characters.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    im = plt.imshow(ohe_matrix, cmap='hot', interpolation='None')\n",
    "    plt.colorbar(im, orientation='horizontal')\n",
    "    plt.xlabel('Length of SMILES string')\n",
    "    plt.ylabel(f'Char in SMILES ({len(smiles_char)})')\n",
    "    plt.title('Visualization of one-hot encoded matrix')\n",
    "    plt.show()\n",
    "    print('Shape of one-hot matrix : ', ohe_matrix.shape)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index\n",
    "index = 0\n",
    "ohe_plot(df.iloc[index]['own_OHE'], SMILES_CHARS)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated canonical SMILES: ', df.iloc[index]['canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "ohe_plot(df.iloc[index]['own_OHE'], SMILES_CHARS)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated canonical SMILES: ', df.iloc[index]['canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, as we notice from the visualizations above, SMILES strings could have unequal dimension since their string length might differ. For machine learning application, having equal dimension throughout the data set is required. In order to achieve this, we first search for the string with the maximum length using the [len()](https://www.geeksforgeeks.org/python-string-length-len/) method. By implementing the above method, we can get the maximum length of the SMILES strings and pass it as an argument in our function for all the strings.\n",
    "\n",
    "- Second, in the above function, we have created a dataset of 56 possible SMILES characters, but to optimize our one-hot encoding, we reconsidered using all the 56 characters. We can assume that all the characters won't be present in our SMILES structures, so we look for all the unique characters present in the SMILES data set.\n",
    "\n",
    "- Third, many elements in the periodic table have two characters in their name. For example 'Cl' stands for chloride. This double-character phenomenon is commonly present in SMILES but if we use the above `smiles_encoder` function then we will be splitting 'Cl' into two characters, namely 'C' and 'l', and that would lead to discrepancies, so searching for each unique character and encoding them may not be optimal. Hence, we search for all the two alphabetic element in our SMILES by comparing the atoms present in our strings with all the possible elements present in the periodic table and replaced all the two alphabetic elements with one character, for example 'Cl' is changed to 'L'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function defined to pre-process the SMILES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function used to preprocessed the data\n",
    "def preprocessing_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the SMILES structures in a data set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "       Dataframe which requires preprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with new processed canonical SMILES column.\n",
    "    unique_char : list\n",
    "        List with unique characters present in SMILES.\n",
    "    SMILES_maxlen : int\n",
    "        Maximum length of SMILES string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate max length of the SMILES strings\n",
    "    SMILES_maxlen = df[\"canonical_smiles\"].str.len().max()\n",
    "\n",
    "    # Search for unique characters in our SMILES strings\n",
    "    unique_char = set(df.canonical_smiles.apply(list).sum())\n",
    "    upper_chars = ['C', 'O', 'F', 'P', 'N', 'S', 'H', 'B', 'I']\n",
    "    lower_chars = ['l', 'o', 'r', 'n', 'e', 'c', 's']\n",
    "\n",
    "    # List of all possible periodic elements\n",
    "    periodic_elements = ['Ac',\n",
    "                         'Al', 'Am', 'Sb', 'Ar', 'As', 'At', 'Ba',\n",
    "                         'Bk', 'Be', 'Bi', 'Bh', 'B', 'Br', 'Cd', 'Ca',\n",
    "                         'Cf', 'C', 'Ce', 'Cs', 'Cl', 'Cr', 'Co', 'Cn',\n",
    "                         'Cu', 'Cm', 'Ds', 'Db', 'Dy', 'Es', 'Er', 'Eu',\n",
    "                         'Fm', 'Fl', 'F', 'Fr', 'Gd', 'Ga', 'Ge', 'Au',\n",
    "                         'Hf', 'Hs', 'He', 'Ho', 'H', 'In', 'I', 'Ir',\n",
    "                         'Fe', 'Kr', 'La', 'Lr', 'Pb', 'Li', 'Lv',\n",
    "                         'Lu', 'Mg', 'Mn', 'Mt', 'Md', 'Hg', 'Mo',\n",
    "                         'Mc', 'Nd', 'Ne', 'Np', 'Ni', 'Nh', 'Nb',\n",
    "                         'N', 'No', 'Og', 'Os', 'O', 'Pd', 'P',\n",
    "                         'Pt', 'Pu', 'Po', 'K', 'Pr', 'Pm', 'Pa',\n",
    "                         'Ra', 'Rn', 'Re', 'Rh', 'Rg', 'Rb', 'Ru', 'Rf',\n",
    "                         'Sm', 'Sc', 'Sg', 'Se', 'Si', 'Ag', 'Na',\n",
    "                         'Sr', 'S', 'Ta', 'Tc', 'Te', 'Ts', 'Tb', 'Tl', 'Th',\n",
    "                         'Tm', 'Sn',\n",
    "                         'Ti', 'W', 'U', 'V', 'Xe', 'Yb', 'Y', 'Zn', 'Zr']\n",
    "\n",
    "    # 'two_charc_elements' is a list that contains two letter elements\n",
    "    # which are valid when compared with all the possible periodic elements.\n",
    "    two_charc_elements = []\n",
    "    for upper in upper_chars:\n",
    "        for lower in lower_chars:\n",
    "            ch = upper + lower\n",
    "            if (ch in periodic_elements):\n",
    "                two_charc_elements.append(ch)\n",
    "\n",
    "    # 'two_charc_elements_smiles' is a set that contains all the possible\n",
    "    # two letter elements in our SMILES strings, that is specific to our dataset.\n",
    "    two_charc_elements_smiles = set()\n",
    "    for char in two_charc_elements:\n",
    "        for index in range(len(df)):\n",
    "            if (df['canonical_smiles'].iloc[index].find(char) != -1):\n",
    "                two_charc_elements_smiles.add(char)\n",
    "    # Create a new column having processed canonical SMILES\n",
    "    df['processed_canonical_smiles'] = \"\"\n",
    "\n",
    "    # Replaced all the two letter elements found\n",
    "    # (Cl, Br, Cn, Se, @@) with one character\n",
    "    for index in range(len(df)):\n",
    "        element = df['canonical_smiles'].iloc[index]\n",
    "        element = element.replace(\"Cl\", \"L\")\n",
    "        element = element.replace(\"Br\", \"R\")\n",
    "        element = element.replace(\"Cn\", \"X\")\n",
    "        element = element.replace(\"Se\", \"Z\")\n",
    "        element = element.replace(\"@@\", \"$\")\n",
    "        df['processed_canonical_smiles'].iloc[index] = element\n",
    "    unique_char = set(df.processed_canonical_smiles.apply(list).sum())\n",
    "    return unique_char, df, SMILES_maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This function is highly specific to the above dataset used, because we have replaced two letter elements that are particularly found in the used dataset. One can add more two letter elements present in their specific SMILES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation on preprocessed data\n",
    "\n",
    "Now we have used the same function `smiles_encoder` but on processed canonical strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function\n",
    "unique_char, df, SMILES_maxlen = preprocessing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of the unique char dataset\n",
    "smi2index = dict((char, index) for index, char in enumerate(unique_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def smiles_encoder(smiles, maxlen):\n",
    "    \"\"\"\n",
    "    Function defined using all unique characters in our\n",
    "    processed canonical smiles structures created\n",
    "    from preprocessed function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "         SMILES data in string format.\n",
    "    maxlen : int\n",
    "         Maximum length of the SMILES string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix : ndarray\n",
    "         One-hot encoded matrix of fixed shape\n",
    "         (unique char in smiles, max smile length).\n",
    "    \"\"\"\n",
    "    smiles_matrix = np.zeros((len(unique_char), maxlen))\n",
    "    for index, char in enumerate(smiles):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the SMILES strings\n",
    "start = timer()\n",
    "df['Unique_char_OHE'] = df['processed_canonical_smiles'].apply(\n",
    "                        smiles_encoder, maxlen=SMILES_maxlen)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "smiles_encoder_equal_dim = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_equal_dim:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index\n",
    "index = 0\n",
    "ohe_plot(df.iloc[index]['Unique_char_OHE'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "ohe_plot(df.iloc[index]['Unique_char_OHE'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualizations, we can conclude that the dimensions are equal for all the strings' one-hot encoded matrix using `preprocessing_data` and `smiles_encoder` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in Scikit-learn\n",
    "Now, we proceed with our second implementation of one-hot encoding from scikit-learn. We can use [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from the `sklearn` library but it only takes numerical categorical values, hence any value of string type should be [label_encoded](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) first before one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the SMILES into list of characters\n",
    "def split(string_data):\n",
    "    \"\"\"\n",
    "    Split the SMILES strings into characters array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string_data : str\n",
    "        SMILES data in string format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of characters in the string data.\n",
    "    \"\"\"\n",
    "    return list(string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-learn implementation of one-hot encoding\n",
    "def sklearn_OHE(smiles, islaterpadding):\n",
    "    \"\"\"\n",
    "    Label and one-hot encodes the SMILES\n",
    "    using sklearn LabelEncoder and OneHotEncoder implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        SMILES string of a compound.\n",
    "    islaterpadding : bool\n",
    "        paramater is `True` if `later_padding` is required,\n",
    "        `False` otherwise.\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    onehot_encoded : ndarray\n",
    "        One-hot encoded matrix of shape\n",
    "        (chars in individual SMILES, length of individual SMILES).\n",
    "    \"\"\"\n",
    "    # Integer encoding\n",
    "    canonical_char = split(smiles)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(canonical_char)\n",
    "    # one-hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    if (islaterpadding == True):\n",
    "        onehot_encoded = later_padding(onehot_encoded)\n",
    "    onehot_encoded = onehot_encoded.transpose()\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions above first give integer encoded SMILES of the labels and finally one-hot encode the SMILES structures.\n",
    "\n",
    "By default, the OneHotEncoder class will return a more efficient sparse encoding which can be useful in some applications but in this case, we disabled the sparse return type by setting the `sparse=False` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)\n",
    "\n",
    "We can use the `sklearn_OHE` function defined above to create the one-hot encoded matrix but this will again create unequal dimensions of the matrix because it will first label encode all the characters present in the SMILES strings (individually) and then one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['sklearn_OHE_no_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                 sklearn_OHE, islaterpadding=False)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_without_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_without_padding:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index\n",
    "index = 0\n",
    "ohe_plot(df.iloc[index]['sklearn_OHE_no_padding'],\n",
    "         df.iloc[index]['sklearn_OHE_no_padding'])  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "ohe_plot(df.iloc[index]['sklearn_OHE_no_padding'],\n",
    "         df.iloc[index]['sklearn_OHE_no_padding'])  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be confirmed from above visualization that the one-hot encoded matrix from the sklearn implementation does not have equal dimensions because sklearn's `OneHotEncoder` function takes individual strings as input.\n",
    "\n",
    "So we equalize dimensions of the one-hot encoded matrix by adding **padding** which simply means adding zeros to it. It can be either done after one-hot encoding is performed on the SMILES strings or before, meaning after we label encode the SMILES characters.\n",
    "\n",
    "We discuss both scenarios in next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding after one-hot encoding is perfomed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined function to add padding after one-hot encoding\n",
    "def later_padding(ohe_matrix):\n",
    "    \"\"\"\n",
    "    Add horizontal and vertical padding\n",
    "    to the given matrix using numpy.pad() function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ohe_matrix: ndarray\n",
    "            Character array.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    padded_matrix: ndarray\n",
    "           Padded one-hot encoded matrix of\n",
    "           shape (unique char in smiles, max smile_length).\n",
    "   \"\"\"\n",
    "    index_padding = np.pad(ohe_matrix, ((0, SMILES_maxlen-len(ohe_matrix)),\n",
    "                                        (0, 0)), 'constant')\n",
    "    column_padding = np.pad(ohe_matrix, ((0, SMILES_maxlen),\n",
    "                                         (0, len(unique_char)-len(\n",
    "                                          ohe_matrix[0]))), 'constant')\n",
    "    ohe_matrix = np.append(ohe_matrix, index_padding, axis=0)\n",
    "    ohe_matrix = np.append(ohe_matrix, column_padding, axis=1)\n",
    "    return ohe_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical smiles strings\n",
    "start = timer()\n",
    "df['sklearn_OHE_later_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                     sklearn_OHE, islaterpadding=True)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_later = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_later:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with Matplotlib.imshow() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIX\n",
    "# y label is not 37\n",
    "# matrices don't have equal dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index\n",
    "index = 0\n",
    "ohe_plot(df.iloc[index]['sklearn_OHE_later_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "ohe_plot(df.iloc[index]['sklearn_OHE_later_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that the dimensions are equal for all the SMILES strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding before one-hot encoding is perfomed\n",
    "\n",
    "In this case we have added padding just after the label encoding or before the one-hot encoding is performed on SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_padding(canonical_char, max_len):\n",
    "    \"\"\"\n",
    "    Add padding to the given list\n",
    "    before one-hot encoding is performed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical_char : array\n",
    "       Character array.\n",
    "    max_len : int\n",
    "       Maximum length of SMILES string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_list : list\n",
    "       Padded character list of SMILES string.\n",
    "    \"\"\"\n",
    "    padded_list = np.pad(canonical_char, (0,\n",
    "                         max_len-len(canonical_char)), 'constant')\n",
    "\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_padding_and_encoding(smiles):\n",
    "    \"\"\"\n",
    "    One-hot encodes initially padded SMILES strings\n",
    "    using sklearn_OneHotEncode function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "       SMILES string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "      One-hot encoded matrix with intial padding of shape\n",
    "      (unique char in smiles, max smile_length).\n",
    "    \"\"\"\n",
    "    # call 'split' function on smiles\n",
    "    canonical_char = split(smiles)\n",
    "    # call 'initial_padding' function on splitted list of characters\n",
    "    canonical_char_padded = initial_padding(canonical_char, SMILES_maxlen)\n",
    "    # call 'sklearn_OHE' function\n",
    "    sklearnOHC = sklearn_OHE(canonical_char_padded, False)\n",
    "    sklearnOHC = sklearnOHC.transpose()\n",
    "    column_padding = np.ndarray(shape=(SMILES_maxlen,\n",
    "                                len(unique_char)-len(sklearnOHC[0])))\n",
    "    column_padding.fill(0)\n",
    "    sklearnOHC = np.append(sklearnOHC, column_padding, axis=1)\n",
    "    return sklearnOHC.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['sklearn_OHE_initial_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                      initial_padding_and_encoding)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_initial = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_initial:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with Matplotlib.imshow() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['sklearn_OHE_initial_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[0]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[100]['sklearn_OHE_initial_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[100]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is also a very powerful and highly used library, especially employed in deep learning tasks. \n",
    "There may be a case where we have sequences or strings that are already integer encoded, then in that case we can use the function called [to_categorical()](https://keras.io/api/utils/) provide by the keras library to one-hot encode integer data directly, but it always should be integer which may not have a real ordinal relationship and are really just placeholders for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keras implementation of one-hot encoding\n",
    "def keras_OHE(smiles, islaterpadding):\n",
    "    \"\"\"\n",
    "    One-hot encodes the SMILES using keras\n",
    "    implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical_char : array\n",
    "        Canonical character array.\n",
    "    islaterpadding : bool\n",
    "         The paramater is `True` if later_padding is required,\n",
    "         `False` otherwise.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    encoded : ndarray\n",
    "        One-hot encoded matrix of shape\n",
    "        (chars in SMILES, length of SMILES).\n",
    "    \"\"\"\n",
    "    # apply 'split' function on smiles string\n",
    "    canonical_char = split(smiles)\n",
    "    # integer encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(canonical_char)\n",
    "    # one-hot encoding\n",
    "    encoded = to_categorical(integer_encoded)\n",
    "    if (islaterpadding == True):\n",
    "        encoded = later_padding(encoded)\n",
    "    encoded = encoded.transpose()\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['keras_OHE_without_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                            keras_OHE, islaterpadding=False)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time_no_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{keras_time_no_padding:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with Matplotlib.imshow() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[0]['keras_OHE_without_padding'],\n",
    "         df.iloc[0]['keras_OHE_without_padding'])  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[0]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_plot(df.iloc[100]['keras_OHE_without_padding'],\n",
    "         df.iloc[100]['keras_OHE_without_padding'])  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[100]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['keras_OHE_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                    keras_OHE, islaterpadding=True)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{keras_time_padding:.2f} secs\")  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with Matplotlib.imshow() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIX\n",
    "# y label is not 37\n",
    "# matrices don't have equal dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index\n",
    "index = 0\n",
    "ohe_plot(df.iloc[index]['keras_OHE_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "ohe_plot(df.iloc[index]['keras_OHE_padding'],\n",
    "         unique_char)  # NBVAL_CHECK_OUTPUT\n",
    "print('Associated processed canonical SMILES: ',\n",
    "      df.iloc[index]['processed_canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe with all the one-hot encoded matrices\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "\n",
    "As we notice from the simulations above, the execution time varies with different implementations:\n",
    "\n",
    "- **Unequal dimension** (when no padding was performed)\n",
    "    - Unexpectedly, our own `smiles_encoder` function works the best with 0.89 sec execution time followed by the implementation in keras with 1.48 sec and one-hot encoding with the scikit-learn implementation executed with 2.42 secs, the highest. But since the dimensions differ, machine learning models cannot be applied.\n",
    "    \n",
    "- **Equal dimension** (when padding was performed)\n",
    "     - Surprisingly, even after creating equal dimensions (adding padding), our own functions `smiles_encoder` along with `preprocessing_data` again outperform the other two implementations with execution time of 1.02 secs whereas scikit-learn required approximately 3.0 secs and keras 1.85 secs. One explanation for this difference may be the additional padding performed on the strings.\n",
    "     \n",
    "     \n",
    "We also stress the time execution difference using scikit-learn when padding is performed:\n",
    "\n",
    "- **Before one-hot encoding** : Time required is 3.0 secs approx.\n",
    "- **After one-hot encoding** : Time required is 2.7 secs approx.\n",
    "\n",
    "One possible reason for this time difference could be that if we perform padding after label encoding, we will have more characters to one hot encode as compared to just pad after one-hot encoding is accomplished.\n",
    "\n",
    "**Note:** Execution times might slighty differ depending on the environment used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were several challenges faced during the task, for instance:\n",
    "\n",
    "- Making equal dimensions of the one-hot encoded matrix.\n",
    "\n",
    "- Replacing two letter elements with unique characters.\n",
    "\n",
    "- After searching for two letter elements, we found a specific element `Sc` which is a metallic element. In our dataset both 'S' and 'c' elements are present individually as well. So if we would have replaced `Sc` with single a letter element then it might have effected our actual strings. So we assumed that since `Sc` is a metallic element and is rarely present in SMILES, we did not replace this element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why is it required to have equal dimensions of the one-hot encoded matrix ?\n",
    "- Is there any other way to pre-process the data ?\n",
    "- How and which machine learning models can be applied on the above dataset ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
